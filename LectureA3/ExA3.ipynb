{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LectureA3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bduAq58dGcIw",
        "colab_type": "text"
      },
      "source": [
        "이 [링크](https://colab.research.google.com/drive/1qIFf2s2MTYc9g1Di2D07fKMq-8oBN8lc)로 들어가 파일->드라이브에 사본 저장으로 드라이브에 똑같은 주피터 노트북을 복사해주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBRafMvr-MzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P42n7at7-gbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def strange_function(x):\n",
        "    # x should be torch.Tensor\n",
        "    # - min(x^2, |4x|) * cos (pi x) + 2 x^2\n",
        "    return - torch.min(x ** 2, torch.abs(4 * x)) * torch.cos(np.pi * x) + 2 * (x ** 2)\n",
        "\n",
        "strange_function(torch.Tensor([1.5])) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUEO3Y--_klu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xs = np.linspace(-10, 10, 1000)\n",
        "\n",
        "plt.plot(xs, strange_function(torch.Tensor(xs)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFMNst_2ACj0",
        "colab_type": "text"
      },
      "source": [
        "### Experiment\n",
        "Your job is to minimize this strange_function. As you can see, it attains its global minimum at x=0, however you should optimize this starting from x>=8. \n",
        "\n",
        "Choose any of gradient descents treated in lecture, then tune learning rate, momentum, and etc.\n",
        "\n",
        "What you should do : \n",
        "\n",
        "1. Attain global minimum x=0.\n",
        "2. Converge in x=0.\n",
        "3. If possible, let epoch small as possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDsbu0vvAakv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.tensor(8.0, requires_grad = True)\n",
        "\n",
        "epoch = 100\n",
        "\n",
        "optim = torch.optim.SGD([x], lr=0.01, weight_decay = 0.01, momentum = 0.01)\n",
        "# optim = torch.optim.Adadelta([x], lr = 0.01)\n",
        "# optim = torch.optim.Adagrad([x], lr = 0.01)\n",
        "# optim = torch.optim.Adam([x], lr = 0.01)\n",
        "# optim = torch.optim.RMSProp([x], lr = 0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBCWALuhCNfb",
        "colab_type": "text"
      },
      "source": [
        "You may find that learning rate change is needed per epoch. You can do it manually, but it is better to use learning rate scheduler implemented in torch. You can see other things that I didn't write in [lr_scheduler](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKw00pS3CMvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lambda x : 0.99 * x) \n",
        "# This does lr -> 0.99 * lr -> 0.99^2 * lr -> ...\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size = 3, gamma = 0.1)\n",
        "# This does lr -> lr * 0.1 -> lr * 0.1^2 per each step_size. i.e., \n",
        "# 0~2     3~5      6~9       10~12   ...\n",
        "#  lr    lr*0.1  lr*0.1^2  lr*0.1^3  ...\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optim, milestones = [5, 10, 20], gamma = 0.1)\n",
        "# This does lr -> lr * 0.1 -> lr * 0.1^2 when met items in milestones. i.e., \n",
        "# 0~4     5~10     11~20      21~\n",
        "# lr     lr*0.1   lr*0.1^2    lr*0.1^3\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.1)\n",
        "# This does lr -> lr * 0.1 every epoch.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csYEAR1tEXX8",
        "colab_type": "text"
      },
      "source": [
        "This code performs final optimizing. I recommend you to not touch this parts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TviD5ClEW2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(epoch):\n",
        "    y = strange_function(x)\n",
        "    y.backward()\n",
        "    grad = x.grad.item()\n",
        "    optim.step()\n",
        "    print(\"Epoch {} : x : {}, y : {}, grad : {}\".format(i, x, y, grad))\n",
        "    scheduler.step()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}