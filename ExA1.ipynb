{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG2ARepTw_Ow",
        "colab_type": "text"
      },
      "source": [
        "이 [링크](https://colab.research.google.com/drive/16MQy7GPrTuzRPqiLxuRt-eQlzeNPVRmT)로 들어가 파일->드라이브에 사본 저장으로 드라이브에 똑같은 주피터 노트북을 복사해주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzZLkuFA8bxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMiYUVDX_G0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.01\n",
        "\n",
        "x = torch.Tensor([1, 2, 3, 4])\n",
        "y = torch.Tensor([1, 2, 3, 4])\n",
        "\n",
        "w = torch.tensor(0.9, requires_grad = True)\n",
        "b = torch.tensor(0.0, requires_grad = True)\n",
        "# requires_grad는 이 값이 optimize될 값이라는 것을 말해줍니다. 정확히는, loss.backward가 이 값에 대한 gradient를 계산하여 저장합니다.\n",
        "\n",
        "for i in range(30):\n",
        "    loss = (((w * x + b) - y) ** 2).sum() / 4\n",
        "    # MLE loss를 계산합니다.\n",
        "    loss.backward()\n",
        "\n",
        "    print(w.grad)\n",
        "    print(b.grad)\n",
        "\n",
        "    w = w - w.grad * lr\n",
        "    b = b - b.grad * lr\n",
        "\n",
        "    w.retain_grad()\n",
        "    b.retain_grad()\n",
        "    # w, b가 새로 업데이트되면서 computation path에서 중간 노드가 되기 때문에 끝 노드라고 명시합니다. \n",
        "    print(w)\n",
        "    print(b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GJjEP_HxkiP",
        "colab_type": "text"
      },
      "source": [
        "### Ex 1 : lots of data\n",
        "\n",
        "데이터의 크기가 매우 큽니다. 이 경우, lr을 작게 잡아도 gradient가 크게 나올 가능성이 있습니다. 설명했던 SGD나 mini batch gradient descent를 생각해서 구현해보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsnKnIa4AuhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "many_x = [i for i in range(10000)]\n",
        "many_y = [x * 4 + 2 + np.random.normal(0, 0.03) for x in many_x]\n",
        "\n",
        "many_x = torch.Tensor(many_x)\n",
        "many_y = torch.Tensor(many_y)\n",
        "# Try this!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3FGTGZmxycb",
        "colab_type": "text"
      },
      "source": [
        "### Ex 2 : Higher polynomial\n",
        "\n",
        "데이터가 $y = 2x^2 + x - 1$의 분포를 따릅니다. 위의 코드를 변형하여 2차 다항식 회귀를 해보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PctcZupEBBxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "high_x = torch.Tensor([-2, -1, 0, 1, 2])\n",
        "high_y = torch.Tensor([4.9, 0.1, -1.1, 2.1, 8.9])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Gj0bgEwx9Ib",
        "colab_type": "text"
      },
      "source": [
        "### Ex 3 : Higher dimension\n",
        "데이터가 $z = x + y + 1$의 분포를 따릅니다. 위의 코드를 변형하여 선형 회귀를 해보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBDBedCIB1k8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multi_x = torch.Tensor([[0, 0], [0, 1], [1, 0], [1, 1], [3, 17]])\n",
        "multi_y = torch.Tensor([1, 2, 2, 3, 21])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}