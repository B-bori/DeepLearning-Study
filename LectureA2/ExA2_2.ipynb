{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LectureA2-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4cTlClxuXah",
        "colab_type": "text"
      },
      "source": [
        "이 [링크](https://colab.research.google.com/drive/1ZG5eD25M8ZI-uKn4jY1olZFMzCLGQU9s)로 들어가 파일->드라이브에 사본 저장으로 드라이브에 똑같은 주피터 노트북을 복사해주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbmejBkAk5bZ",
        "colab_type": "text"
      },
      "source": [
        "### 1. Importing required packages and fixing random seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxGG6JdXk4_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "import os\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "import time\n",
        "from itertools import product\n",
        "\n",
        "manual_seed = \"aiming\".__hash__() % (2 ** 32) #random.randint(1, 10000)\n",
        "print(\"Random Seed: \", manual_seed)\n",
        "random.seed(manual_seed)\n",
        "torch.manual_seed(manual_seed)\n",
        "np.random.seed(manual_seed)\n",
        "\n",
        "!mkdir results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O03fUAXGk_q6",
        "colab_type": "text"
      },
      "source": [
        "### 2. Downloading Dataset\n",
        "For first example, we will use MNIST dataset. It is image data set consisting handwritting of numbers, 0-9. The data is $28\\times 28$ pixel grayscale image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_pWgpKllDcZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = torchvision.transforms.Compose(\n",
        "    [torchvision.transforms.ToTensor(), \n",
        "     torchvision.transforms.Normalize((0,), (1,))])\n",
        "# Need to add normalize\n",
        "trainset = torchvision.datasets.MNIST(root='../data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "# This contains total 60,000 data.\n",
        "testset = torchvision.datasets.MNIST(root='../data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "# This contains total 10,000 data. You SHOULD NOT train with these data.\n",
        "trainset, valset = torch.utils.data.random_split(trainset, [50000, 10000])\n",
        "partition = {'train':trainset, 'val': valset, 'test':testset}\n",
        "\n",
        "print(len(partition['train']))\n",
        "print(len(partition['val']))\n",
        "print(len(partition['test']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1CjbyYplWqT",
        "colab_type": "text"
      },
      "source": [
        "### 3. Model Construction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo1MG7UTlGKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, hid_dim, n_layer, act, dropout_rate, batchnorm, init):\n",
        "        super(VanillaMLP, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layer = n_layer\n",
        "        self.act = act\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.batchnorm = batchnorm\n",
        "        self.init = init\n",
        "\n",
        "        # ====== Create Linear Layers ====== #\n",
        "        self.lin_first = nn.Linear(self.in_dim, self.hid_dim)\n",
        "        self.linears = nn.ModuleList()\n",
        "        for i in range(self.n_layer - 1):\n",
        "            self.linears.append(nn.Linear(self.hid_dim, self.hid_dim))\n",
        "        self.lin_last = nn.Linear(self.hid_dim, self.out_dim)\n",
        "\n",
        "        # ====== Create Activation Function ====== #\n",
        "        if self.act.lower() == \"sigmoid\":\n",
        "            self.activation = nn.Sigmoid()\n",
        "        elif self.act.lower() == \"tanh\":\n",
        "            self.activation = nn.Tanh()\n",
        "        elif self.act.lower() == \"relu\":\n",
        "            self.activation = nn.ReLU()\n",
        "        else:\n",
        "            raise ValueError(\"Not a valid activation function argument\")\n",
        "\n",
        "        # ====== Create BatchNorm, Dropout Layers ====== #\n",
        "        self.bn_first = nn.BatchNorm1D(self.hid_dim)\n",
        "        self.batchnorms = nn.ModuleList()\n",
        "        if self.batchnorm:\n",
        "            for i in range(self.n_layer - 1):\n",
        "                self.batchnorms.append(nn.BatchNorm1D(self.hid_dim))\n",
        "        \n",
        "        self.dropout = nn.Dropout(self.dropout_rate)\n",
        "\n",
        "        if self.init == \"normal\":\n",
        "            if self.act.lower() == \"relu\":\n",
        "                self.weight_init(nn.init.kaiming_normal_)\n",
        "            else:\n",
        "                self.weight_init(nn.init.xavier_normal_)\n",
        "        elif self.init == \"uniform\":\n",
        "            if self.act.lower() == \"relu\":\n",
        "                self.weight_init(nn.init.kaiming_uniform_)\n",
        "            else:\n",
        "                self.weight_init(nn.init.xavier_uniform_)\n",
        "        \n",
        "    def forward(self, x, is_train):\n",
        "        # Ordering : FC -> BatchNorm -> act -> dropout\n",
        "        # I referred this stackoverflow answer for ordering. \n",
        "        # https://stackoverflow.com/a/40295999\n",
        "        \n",
        "        x = x.view(-1, self.in_dim)\n",
        "        x = self.lin_first(x)\n",
        "        if self.batchnorm:\n",
        "            x = self.bn_first(x)\n",
        "        x = self.activation(x)\n",
        "        if self.dropout_rate != 0.0 and is_train:\n",
        "            x = self.dropout(x)\n",
        "        for i in range(self.n_layer - 1):\n",
        "            if self.batchnorm:\n",
        "                x = self.batchnorms[i](x)\n",
        "            x = self.activation(x)\n",
        "            if self.dropout_rate != 0.0 and is_train:\n",
        "                x = self.dropout(x)\n",
        "        return self.lin_last(x)\n",
        "\n",
        "    def weight_init(self, initializer):\n",
        "        initializer(self.lin_first.weight)\n",
        "        self.lin_first.bias.data.fill_(0.0)\n",
        "        for linear in self.linears:\n",
        "            initializer(linear.weight)\n",
        "            linear.bias.data.fill_(0.0)\n",
        "\n",
        "test_net = MLP(784, 10, 300, 3, \"relu\", 0.2, True, \"normal\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRAR-BKLrZbu",
        "colab_type": "text"
      },
      "source": [
        "### 4. Defining Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfKacKmSrZ57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "\n",
        "# Training related\n",
        "args.batch_size = 128\n",
        "args.test_batch_size = 1000\n",
        "args.learning_rate = .01\n",
        "args.epoch = 5\n",
        "args.weight_decay = 0.1\n",
        "\n",
        "# Model related\n",
        "args.n_layer = 3\n",
        "args.hid_dim = 100\n",
        "args.act = \"relu\"\n",
        "\n",
        "# Fixed parameter\n",
        "args.in_dim = 784\n",
        "args.out_dim = 10\n",
        "\n",
        "# Regularization related\n",
        "self.dropout_rate = 0.1\n",
        "self.batchnorm = True\n",
        "self.init = \"normal\"\n",
        "\n",
        "print(args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56MT2poIr9hB",
        "colab_type": "text"
      },
      "source": [
        "### 5. Defining helper function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9kp0lRVsAMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, partition, optimizer, criterion, args):\n",
        "    trainloader = torch.utils.data.DataLoader(partition['train'], \n",
        "                                              batch_size=args.batch_size, \n",
        "                                              shuffle=True, num_workers=2)\n",
        "    net.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    train_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.view(-1, args.in_dim)\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = train_loss / len(trainloader)\n",
        "    train_acc = 100 * correct / total\n",
        "    return net, train_loss, train_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4Je6oNdsDdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(net, partition, criterion, args):\n",
        "    valloader = torch.utils.data.DataLoader(partition['val'], \n",
        "                                            batch_size=args.test_batch_size, \n",
        "                                            shuffle=False, num_workers=2)\n",
        "    net.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0 \n",
        "    with torch.no_grad():\n",
        "        for data in valloader:\n",
        "            images, labels = data\n",
        "            images = images.view(-1, args.in_dim)\n",
        "            outputs = net(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(valloader)\n",
        "        val_acc = 100 * correct / total\n",
        "    return val_loss, val_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7mskencsFvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(net, partition, args):\n",
        "    testloader = torch.utils.data.DataLoader(partition['test'], \n",
        "                                             batch_size=args.test_batch_size, \n",
        "                                             shuffle=False, num_workers=2)\n",
        "    net.eval()\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images = images.view(-1, args.in_dim)\n",
        "\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_acc = 100 * correct / total\n",
        "    return test_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXi_t0ytsIIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def experiment(partition, args):\n",
        "    \n",
        "    net = MLP(args.in_dim, args.out_dim, args.hid_dim, args.n_layer, args.act, args.dropout_rate, args.batchnorm, args.init)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr = args.learning_rate, weight_decay = args.weight_decay)\n",
        "    \n",
        "    print(args)\n",
        "\n",
        "    train_losses = list()\n",
        "    val_losses = list()\n",
        "    train_accs = list()\n",
        "    val_accs = list()\n",
        "    test_accs = list()\n",
        "\n",
        "    \n",
        "    for epoch in range(args.epoch):\n",
        "        ts = time.time()\n",
        "        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n",
        "        val_loss, val_acc = validate(net, partition, criterion, args)\n",
        "        te = time.time()\n",
        "        \n",
        "        print('Epoch {}, Acc(train/val) : {:2.2f}/{:2.2f}, Loss(train/val): {:2.2f}/{:2.2f}. Took {:2.2f} sec.'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
        "        \n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "        \n",
        "    test_acc = test(net, partition, args)\n",
        "\n",
        "    result = {}\n",
        "    result['train_losses'] = train_losses\n",
        "    result['train_accs'] = train_accs\n",
        "    result['val_losses'] = val_losses\n",
        "    result['val_accs'] = val_accs\n",
        "    result['test_acc'] = test_acc\n",
        "    \n",
        "    return vars(args), result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3IjJDB3siLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import hashlib\n",
        "import json\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "\n",
        "def save_exp_result(setting, result):\n",
        "    exp_name = setting['exp_name']\n",
        "    del setting['epoch']\n",
        "    del setting['test_batch_size']\n",
        "\n",
        "    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
        "    filename = './results/{}-{}.json'.format(exp_name, hash_key)\n",
        "    result.update(setting)\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(result, f)\n",
        "    \n",
        "def load_exp_result(exp_name):\n",
        "    dir_path = './results'\n",
        "    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n",
        "    list_result = []\n",
        "    for filename in filenames:\n",
        "        if exp_name in filename:\n",
        "            with open(join(dir_path, filename), 'r') as infile:\n",
        "                results = json.load(infile)\n",
        "                list_result.append(results)\n",
        "    df = pd.DataFrame(list_result) # .drop(columns=[])\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6c1Tz2TskWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "def download_local(exp_name):\n",
        "    dir_path = './results'\n",
        "    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if exp_name in f if '.json' in f]\n",
        "    for filename in filenames:\n",
        "        files.download(dir_path + \"/\" + filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5XxO5dZtrCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_to_dict(args, name_var1, name_var2, list_var1, list_var2):\n",
        "    loss_dict = dict()\n",
        "    acc_dict = dict()\n",
        "    df = load_exp_result(args.exp_name)\n",
        "\n",
        "    for var1 in list_var1:\n",
        "        for var2 in list_var2:\n",
        "            row = df.loc[df[name_var1]==var1]\n",
        "            row = row.loc[df[name_var2]==var2]\n",
        "            dl = list()\n",
        "            da = list()\n",
        "            for d in row.train_losses:\n",
        "                dl.append(d)\n",
        "            for d in row.train_accs:\n",
        "                da.append(d)\n",
        "            loss_dict[(var1, var2, 'train')] = (list(range(args.epoch)), dl)\n",
        "            acc_dict[(var1, var2, 'train')] = (list(range(args.epoch)), da)\n",
        "            dl = list()\n",
        "            da = list()\n",
        "            for d in row.val_losses:\n",
        "                dl.append(d)\n",
        "            for d in row.val_accs:\n",
        "                da.append(d)\n",
        "            loss_dict[(var1, var2, 'val')] = (list(range(args.epoch)), dl)\n",
        "            acc_dict[(var1, var2, 'val')] = (list(range(args.epoch)), da)\n",
        "\n",
        "    return loss_dict, acc_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG4_61nOtsut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dict_to_plot(args, data_dict, name_var1, name_var2, list_var1, list_var2):\n",
        "    f, axes = plt.subplots(len(list_var1), len(list_var2), sharey=True)\n",
        "\n",
        "    for i in range(len(list_var1)):\n",
        "        for j in range(len(list_var2)):\n",
        "            axes[i][j].plot(list(range(args.epoch)), data_dict[list_var1[i], list_var2[j], 'train'][1][0], '-b')\n",
        "            axes[i][j].plot(list(range(args.epoch)), data_dict[list_var1[i], list_var2[j], 'val'][1][0], '-r')\n",
        "            \n",
        "    for i in range(len(list_var1)):\n",
        "        if len(list_var1) // 2 == i:\n",
        "            axes[i][0].set_ylabel(name_var1 + \"\\n\" + str(list_var1[i]))\n",
        "        else:\n",
        "            axes[i][0].set_ylabel(list_var1[i])\n",
        "    for j in range(len(list_var2)):\n",
        "        if len(list_var2) // 2 == j:\n",
        "            axes[len(list_var1) - 1][j].set_xlabel(str(list_var2[i]) + \"\\n\" + name_var2)\n",
        "        else:\n",
        "            axes[len(list_var1) - 1][j].set_xlabel(list_var2[j])\n",
        "    return f, axes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlVp4Qfhshvu",
        "colab_type": "text"
      },
      "source": [
        "### 6. Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O27m5ETuss1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args.exp_name = \"exp1\"\n",
        "\n",
        "name_var1 = \"n_layer\"\n",
        "name_var2 = \"hid_dim\"\n",
        "list_var1 = [1, 2]\n",
        "list_var2 = [100, 200]\n",
        "\n",
        "for var_list in product(list_var1, list_var2):\n",
        "    setattr(args, name_var1, var_list[0])\n",
        "    setattr(args, name_var2, var_list[1])\n",
        "    setting, result = experiment(partition, deepcopy(args))\n",
        "    save_exp_result(setting, result)\n",
        "\n",
        "# download_local(\"exp1\")\n",
        "# loss_dict, acc_dict = data_to_dict(args.exp_name)\n",
        "# f1, axes1 = dict_to_plot(loss_dict)\n",
        "# f2, axes2 = dict_to_plot(acc_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}